# MainkurafutoAI のしくみ (Architecture)

このAIがどのようにマインクラフトをプレイしているのか、人間になぞらえて解説します。

## 概要: 目・脳・手

AIは大きく分けて**3つの部分**で構成されています。

```mermaid
graph TD
    Minecraft[Minecraft画面] --> Eyes[目 (Vision Layer)]
    Eyes --> Reflex[反射脳 (Reflex Layer)]
    Eyes --> Thinking[思考脳 (Skills/Planning)]
    
    Reflex --> Arbitrator{行動調停}
    Thinking --> Arbitrator
    
    Arbitrator --> Hand[手 (Input Controller)]
    Hand --> Game[仮想コントローラー操作]
```

---

## 1. 目 (Vision Layer)
**「画面を見て、なにがあるか理解する」**

- **スクリーンキャプチャ**: 1秒間に数回、画面の写真を撮ります。
- **YOLO (物体認識AI)**: 写真の中から「ゾンビ」「人」「アイテム」などをAIが見つけ出し、箱（バウンディングボックス）で囲みます。
- **OpenCV (色認識)**: 画面の下の方に「明るいオレンジ色」が広がっていたら「溶岩だ！」と判断します。

## 2. 脳 (Brain)
**「見た情報をもとに、どうするか決める」**

このAIには2つの脳があります。

### ⚡ 反射脳 (Reflex Layer)
**「危ない！逃げろ！」**
- **役割**: 生存最優先。深く考えずに反応します。
- **動作**: 溶岩を見つけた瞬間、今やっていることを全てキャンセルして「後ろに下がる」命令を出します。

### 🧠 思考脳 (Skill & Planning Layer)
**「敵がいるから倒そう」「釣りをしよう」**
- **Combat Skill**: YOLOが見つけた敵の位置を計算し、画面の中央に捉えるように視点を動かします（エイムボット）。捉えたら攻撃します。
- **Fishing Skill**: 画面の特定エリアを監視し、ピクセルの変化（水しぶき）があったら右クリックします。
- **LLM (計画)**: Google GeminiなどのチャットAIを使って、「鉄を集めて」といった言葉の指示を「移動→掘る」といった具体的な行動リストに変換します。

## 3. 手 (Input Controller)
**「決まったことを実行する」**

- パソコンの中に**仮想のXboxコントローラー**を作り出します。
- 脳からの「前へ進め」「Aボタンを押せ」という命令を受け取り、本物のコントローラーが操作されたかのような信号をマインクラフトに送ります。

---

## 起動の仕組み (Context Menu Launch)

今回追加する「右クリック起動」は、Windowsのレジストリという設定場所に、
「背景を右クリックしたら、AIの起動コマンド (`python main.py`) を実行するボタンを表示してね」
という指示を書き込むものです。
